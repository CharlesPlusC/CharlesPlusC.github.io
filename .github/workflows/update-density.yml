name: Update TLE Density Data

on:
  schedule:
    # Run every 8 hours
    - cron: '0 */8 * * *'
  workflow_dispatch: # Allow manual trigger

jobs:
  update-density:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install requests numpy

      - name: Fetch TLEs and calculate densities
        run: |
          python3 << 'EOF'
          import requests
          import json
          import numpy as np
          from datetime import datetime, timedelta, timezone
          import os
          import time

          # Constants
          MU = 398600.4418e9       # Earth's gravitational parameter in m³/s²
          R_EARTH = 6378137.0      # Earth radius in m

          # Satellite configurations
          SATELLITES = {
              '43476': {
                  'name': 'GRACE-FO-A',
                  'cd': 2.2,
                  'area': 3.0,
                  'mass': 580.0
              },
              '43877': {
                  'name': 'Kanopus-V 6',
                  'cd': 2.2,
                  'area': 2.0,
                  'mass': 500.0
              },
              '39212': {
                  'name': 'CZ-4C DEB',
                  'cd': 2.2,
                  'area': 1.0,
                  'mass': 50.0
              },
              '48714': {
                  'name': 'NOAA 17 DEB',
                  'cd': 2.2,
                  'area': 0.5,
                  'mass': 25.0
              },
              '64631': {
                  'name': 'CZ-6A DEB',
                  'cd': 2.2,
                  'area': 1.0,
                  'mass': 50.0
              }
          }

          def parse_mean_motion(line2):
              return float(line2[52:63])

          def parse_mean_motion_derivative(line1):
              raw = line1[33:43].strip()
              return float(raw)

          def parse_eccentricity(line2):
              return float("0." + line2[26:33])

          def tle_epoch_to_datetime(line1):
              epoch = line1[18:32]
              year_2digit = int(epoch[:2])
              year = 2000 + year_2digit if year_2digit < 57 else 1900 + year_2digit
              day = float(epoch[2:])
              dt = datetime(year, 1, 1, tzinfo=timezone.utc) + timedelta(days=day - 1)
              return dt

          def calculate_density_and_orbit(line1, line2, cd, area, mass):
              try:
                  epoch = tle_epoch_to_datetime(line1)
                  N = parse_mean_motion(line2)
                  N_dot = parse_mean_motion_derivative(line1)

                  n = N * 2 * np.pi / 86400.0
                  n_dot = N_dot * 2 * np.pi / (86400.0**2)

                  a = (MU / n**2)**(1/3)
                  e = parse_eccentricity(line2)
                  perigee_alt = (a * (1 - e) - R_EARTH) / 1000
                  apogee_alt = (a * (1 + e) - R_EARTH) / 1000

                  if perigee_alt < 100 or perigee_alt > 2000:
                      return None

                  v = np.sqrt(MU / a)
                  rho = (2 * mass * n_dot) / (3 * cd * area * n * v)

                  if rho < 0:
                      return None

                  return {
                      'epoch': epoch.isoformat(),
                      'density': float(rho),
                      'perigee': float(perigee_alt),
                      'apogee': float(apogee_alt),
                      'sma': float(a / 1000)
                  }
              except Exception as e:
                  print(f"    Error parsing TLE: {e}")
                  return None

          def fetch_current_tle(norad_id, max_retries=3):
              url = f"https://celestrak.org/NORAD/elements/gp.php?CATNR={norad_id}&FORMAT=TLE"

              for attempt in range(max_retries):
                  try:
                      response = requests.get(url, timeout=30)
                      if response.status_code == 200:
                          lines = response.text.strip().split('\n')
                          if len(lines) >= 2:
                              if len(lines) >= 3 and not lines[0].startswith('1 '):
                                  return [(lines[1].strip(), lines[2].strip())]
                              else:
                                  return [(lines[0].strip(), lines[1].strip())]
                  except Exception as e:
                      print(f"    Attempt {attempt + 1} failed: {e}")
                      if attempt < max_retries - 1:
                          time.sleep(2 ** attempt)
              return []

          def main():
              print("=" * 60)
              print("TLE DENSITY INVERSION CALCULATOR")
              print("=" * 60)

              os.makedirs('data', exist_ok=True)

              for norad_id, sat_config in SATELLITES.items():
                  print(f"\nProcessing: {sat_config['name']} (NORAD {norad_id})")
                  print("-" * 40)

                  tles = fetch_current_tle(norad_id)

                  if not tles:
                      print(f"  No TLE data retrieved for {norad_id}")
                      continue

                  # Load existing data
                  existing_data = {'times': [], 'densities': [], 'perigees': [], 'apogees': [], 'smas': []}
                  data_file = f"data/density-{norad_id}.json"

                  if os.path.exists(data_file):
                      try:
                          with open(data_file, 'r') as f:
                              existing_data = json.load(f)
                          print(f"    Loaded {len(existing_data.get('times', []))} existing data points")
                      except Exception as e:
                          print(f"    Could not load existing data: {e}")

                  existing_times = set(existing_data.get('times', []))

                  new_count = 0
                  for line1, line2 in tles:
                      result = calculate_density_and_orbit(
                          line1, line2,
                          sat_config['cd'],
                          sat_config['area'],
                          sat_config['mass']
                      )

                      if result and result['epoch'] not in existing_times:
                          existing_data.setdefault('times', []).append(result['epoch'])
                          existing_data.setdefault('densities', []).append(result['density'])
                          existing_data.setdefault('perigees', []).append(result['perigee'])
                          existing_data.setdefault('apogees', []).append(result['apogee'])
                          existing_data.setdefault('smas', []).append(result['sma'])
                          new_count += 1

                  print(f"  Added {new_count} new data points")
                  print(f"  Total data points: {len(existing_data.get('times', []))}")

                  # Sort by time
                  if existing_data.get('times'):
                      combined = list(zip(
                          existing_data['times'],
                          existing_data['densities'],
                          existing_data['perigees'],
                          existing_data['apogees'],
                          existing_data['smas']
                      ))
                      combined.sort(key=lambda x: x[0])

                      existing_data['times'] = [x[0] for x in combined]
                      existing_data['densities'] = [x[1] for x in combined]
                      existing_data['perigees'] = [x[2] for x in combined]
                      existing_data['apogees'] = [x[3] for x in combined]
                      existing_data['smas'] = [x[4] for x in combined]

                  # Add metadata
                  existing_data['norad_id'] = norad_id
                  existing_data['name'] = sat_config['name']
                  existing_data['cd'] = sat_config['cd']
                  existing_data['area'] = sat_config['area']
                  existing_data['mass'] = sat_config['mass']
                  existing_data['generated_at'] = datetime.now(timezone.utc).isoformat()

                  # Save results
                  with open(data_file, 'w') as f:
                      json.dump(existing_data, f)
                  print(f"  Saved to {data_file}")

              print("\n" + "=" * 60)
              print("Processing complete!")
              print("=" * 60)

          if __name__ == "__main__":
              main()
          EOF

      - name: Fetch Kp index data
        run: |
          python3 << 'EOF'
          import json
          import urllib.request
          from datetime import datetime, timezone

          print("=" * 60)
          print("FETCHING Kp INDEX DATA")
          print("=" * 60)

          # Fetch from GFZ Potsdam
          url = "https://kp.gfz.de/app/files/Kp_ap_Ap_SN_F107_since_1932.txt"

          try:
              with urllib.request.urlopen(url, timeout=60) as response:
                  content = response.read().decode('utf-8')
          except Exception as e:
              print(f"Failed to fetch GFZ data: {e}")
              exit(0)  # Don't fail the workflow

          # Parse data - get current year and previous year
          current_year = datetime.now().year
          times = []
          values = []

          for line in content.split('\n'):
              if not line.strip() or line.startswith('#'):
                  continue

              parts = line.split()
              if len(parts) < 14:
                  continue

              try:
                  year = int(parts[0])
                  month = int(parts[1])
                  day = int(parts[2])

                  # Get current and previous year
                  if year < current_year - 1:
                      continue

                  # Kp values in columns 7-14 (8 values per day, 0-indexed: 7-14)
                  for i, hour in enumerate([0, 3, 6, 9, 12, 15, 18, 21]):
                      kp_value = float(parts[7 + i])
                      dt = datetime(year, month, day, hour, 0, 0, tzinfo=timezone.utc)
                      times.append(dt.strftime('%Y-%m-%d %H:%M:%S.000'))
                      values.append(kp_value)
              except (ValueError, IndexError):
                  continue

          print(f"Parsed {len(times)} Kp values")

          if times:
              print(f"Date range: {times[0]} to {times[-1]}")

              kp_data = {
                  'times': times,
                  'values': values,
                  'source': 'GFZ Potsdam',
                  'generated_at': datetime.now(timezone.utc).isoformat()
              }

              with open('data/kp-index.json', 'w') as f:
                  json.dump(kp_data, f)

              print("Saved to data/kp-index.json")
          else:
              print("No Kp data found")

          print("=" * 60)
          EOF

      - name: Commit and push if changed
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          git add data/density-*.json data/kp-index.json

          if git diff --staged --quiet; then
            echo "No changes to data"
          else
            git commit -m "Update TLE density and Kp index data [automated]"
            git push
            echo "Pushed updated data"
          fi
